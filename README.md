# Python based RAG Pipeline locally with Ollama, llama3, nomic-embed-text

What is RAG?

![RAG is a hybrid approach tha](https://github.com/user-attachments/assets/4f76661e-b4f8-4744-ae08-c99e5cbe3224)

RAG, which stands for Retrieval Augmented Generation, is a technique used in conjunction with Large Language Models (LLMs) to improve their performance, especially in generating accurate and relevant responses or information.

RAG is a hybrid approach that combines two main components:

Retrieval Component: This involves fetching relevant documents or information from a large database or corpus. The retrieval system uses a query (often generated from the input prompt or question) to find the most relevant pieces of information from a set of documents, knowledge bases, or other sources.
Generation Component: This involves a generative model, typically a pre-trained LLM like GPT Open AI models, Ollama hosted LLM(s). The generative model uses the information retrieved in the first step to produce a coherent and contextually appropriate response.

![RAG-Pre-Process](https://github.com/user-attachments/assets/c1105c90-3708-4615-ace1-b94f3d3a78be)



![RAG-Retrieve   Generate](https://github.com/user-attachments/assets/a3bb1cc1-7da1-49f7-8117-f2894963433c)


# Explanation
https://medium.com/@rahul.dusad/run-rag-pipeline-locally-with-ollama-embedding-model-nomic-embed-text-generate-model-llama3-e7a554a541b3
