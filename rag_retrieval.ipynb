{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f97578d2-a856-47e2-8838-72c0dad6afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66f58681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents fetched from database : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ollama_host_url = \"http://localhost:11434\"\n",
    "local_model = \"llama3\"\n",
    "embedding_model = \"nomic-embed-text\"\n",
    "client = Client(host=ollama_host_url)\n",
    "\n",
    "#load the chroma vector database created in previous step using same embeddings model.\n",
    "vector_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=OllamaEmbeddings(model=embedding_model, base_url=ollama_host_url,show_progress=True), collection_name=\"local_rag_db\")\n",
    "retriever = vector_db.as_retriever()\n",
    "question = \"What is banker player?\"\n",
    "docs = retriever.invoke(question)\n",
    "print(\"Documents fetched from database : \"+str(len(docs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b6281a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the Banker is a player who will also make a good Auctioneer. They are responsible for keeping their personal funds separate from those of the Bank. When there are more than five players, the Banker may elect to act only as the Banker and Auctioneer.\n"
     ]
    }
   ],
   "source": [
    "# Join all the documents fetched from database\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "# Create a RAG prompt in below format\n",
    "formatted_prompt = f\"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# Call ollama chat api to generate the response from provided context\n",
    "response = client.chat(model='llama3',messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
